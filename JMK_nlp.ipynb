{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee36cad0-e3a3-4fc7-b5fd-bc71fc043dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAI MAA KALI_/\\_\n",
      "JAI MAA KALI_/\\_\n"
     ]
    }
   ],
   "source": [
    "print('JAI MAA KALI_/\\_')\n",
    "print('JAI MAA KALI_/\\_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d312423d-cc28-47a5-bac6-efc991a311fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4f89e9-05dd-4ebf-a45e-233628430389",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e50c842-867e-4368-9267-951cae25b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0062fdaa-7718-4989-b6ad-9cc0f3a47a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed5ca5f2-6bf3-4f17-8a63-33be49dd51c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', 'love', 'you', 'mother']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('hey love you mother')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53deed66-7ac7-4980-a9db-1dcd8b99146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3929a8e-9c22-4910-b486-43aca2431979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', 'love', 'you', 'mother']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.tokenize('hey love you mother')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f4148a-712f-4d2f-85ea-efeae598774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['Hello! NLP is fun', \" Let's learn tokenization\", '']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello! NLP is fun. Let's learn tokenization.\"\n",
    "\n",
    "# Splitting words based on space\n",
    "words = text.split('.')\n",
    "print(\"Word Tokenization:\", words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0c3c5f3-d824-4638-9999-6c13b250274f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SoMaJo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSoMaJo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SoMaJo\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello!!! üòä This is NLP. Can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt wait to learn more... #Excited\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the SoMaJo tokenizer (English model)\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'SoMaJo'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement SoMoJo (from versions: none)\n",
      "ERROR: No matching distribution found for SoMoJo\n"
     ]
    }
   ],
   "source": [
    "from SoMaJo import SoMaJo\n",
    "\n",
    "text = \"Hello!!! üòä This is NLP. Can't wait to learn more... #Excited\"\n",
    "\n",
    "# Load the SoMaJo tokenizer (English model)\n",
    "tokenizer = SoMaJo(\"en_PTB\")\n",
    "\n",
    "# Tokenize text\n",
    "tokens = tokenizer.tokenize_text([text])\n",
    "\n",
    "# Print word tokenization\n",
    "print(\"Word Tokenization:\")\n",
    "for sentence in tokens:\n",
    "    for token in sentence:\n",
    "        print(token.text)  # Extracting each token\n",
    "\n",
    "# Print sentence tokenization\n",
    "print(\"\\nSentence Tokenization:\")\n",
    "for sentence in tokens:\n",
    "    print(\" \".join([token.text for token in sentence]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ecec2d1-94f3-4186-a6bf-5e6481aef974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SoMoJo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a4be729-8a6a-4d22-a005-7f17667445d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"hi,hello, this is sunny,i saw that your playing badminton in the garden\"\n",
    "data=txt.split(' ')\n",
    "fdist1=nltk.FreqDist(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af12dbbc-af50-4dc1-b0b7-47259a6f33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdb2b955-5b19-4ec8-a7fb-19fed1a56ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a1bf135-bc13-4ace-ac47-b1dc3c1422e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b71d07a9-ac73-47d8-ac85-9567c2acad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Word  Frequency\n",
      "2  learning          3\n",
      "6    python          3\n",
      "0       fun          1\n",
      "1     hello          1\n",
      "3   machine          1\n",
      "4       nlp          1\n",
      "5  powerful          1\n",
      "7    really          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example corpus of text (list of sentences or documents)\n",
    "corpus = [\n",
    "    \"Hello, I'm learning Python.\",\n",
    "    \"Python is really fun and Python is powerful.\",\n",
    "    \"I am learning machine learning and NLP.\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer with stop words removal\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the corpus to a term-document matrix (bag of words)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words) from the vocabulary\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the word counts (matrix) and sum the occurrences of each word\n",
    "word_counts = X.toarray()\n",
    "\n",
    "# Sum the word counts across all documents (axis=0 means sum columns)\n",
    "word_freq = word_counts.sum(axis=0)\n",
    "\n",
    "# Create a DataFrame from the word frequencies\n",
    "df_word_freq = pd.DataFrame({'Word': words, 'Frequency': word_freq})\n",
    "\n",
    "# Sort the DataFrame by frequency in descending order\n",
    "df_word_freq = df_word_freq.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d816e19d-7ca8-4ea9-9a87-dfcb96a188f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 2, 1],\n",
       "       [0, 0, 2, 1, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aef5d67b-378c-4fb7-866a-4c0c0572dcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English :  ['The', 'world', 'is', 'healing', 'now']\n",
      "Russian :  ['–ú–∏—Ä', '–∏—Å—Ü–µ–ª—è–µ—Ç—Å—è', '—Å–µ–π—á–∞—Å']\n",
      "French :  ['Le', 'monde', 'gu√©rit', 'maintenant']\n"
     ]
    }
   ],
   "source": [
    "from nltk import ToktokTokenizer\n",
    "toktok = ToktokTokenizer()\n",
    "# English\n",
    "text = 'The world is healing now'\n",
    "print(\"English : \", toktok.tokenize(text))\n",
    "\n",
    "# Russian\n",
    "text = '–ú–∏—Ä –∏—Å—Ü–µ–ª—è–µ—Ç—Å—è —Å–µ–π—á–∞—Å'\n",
    "print(\"Russian : \", toktok.tokenize(text))\n",
    "# French\n",
    "text = 'Le monde gu√©rit maintenant'\n",
    "print(\"French : \", toktok.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48167baa-b091-4f74-847c-9cfaf56f1346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms for 'happy':\n",
      "['happy', 'well-chosen', 'glad', 'felicitous']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Function to get synonyms from WordNet\n",
    "def get_synonyms(word):\n",
    "    # Get synsets for the word\n",
    "    synsets = wn.synsets(word)\n",
    "    \n",
    "    # List to store synonyms\n",
    "    synonyms = set()\n",
    "\n",
    "    # Iterate over the synsets and extract synonyms\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())  # Adding synonym to the set\n",
    "    \n",
    "    return list(synonyms)\n",
    "\n",
    "# Example: Get synonyms for the word \"happy\"\n",
    "word = \"happy\"\n",
    "synonyms = get_synonyms(word)\n",
    "\n",
    "# Print the synonyms\n",
    "print(f\"Synonyms for '{word}':\")\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9a77df5d-7f6b-4eb2-86fd-c13cb79a382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=wn.synsets('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "99983e4f-a6f7-45b6-a7f5-bba0901a7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=a[1].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3f221a69-817d-4c9c-99a6-e3fcc8d8d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felicitous\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "for i in t:\n",
    "    print(i.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2df20db9-7127-407f-8195-9a916faa8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(word):\n",
    "    z=wn.synsets(word)\n",
    "    f=set()\n",
    "    for i in z:\n",
    "        x=i.lemmas()\n",
    "        for j in x:\n",
    "            f.add(j.name())\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d9c3ff0c-0e73-4b1d-aa4c-ef51d62df368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'felicitous', 'glad', 'happy', 'well-chosen'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4903bf47-9498-4a9f-a2e6-b23be7417a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=wn.synsets('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "452c47e2-2d2b-4314-b8fa-6e0fb784a2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('happy.a.01.happy')]\n",
      "[Lemma('felicitous.s.02.felicitous'), Lemma('felicitous.s.02.happy')]\n",
      "[Lemma('glad.s.02.glad'), Lemma('glad.s.02.happy')]\n",
      "[Lemma('happy.s.04.happy'), Lemma('happy.s.04.well-chosen')]\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i.lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "354b123a-d3bb-4548-be66-2f71b0558799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antonyms for 'happy':\n",
      "['unhappy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Function to get antonyms from WordNet\n",
    "def get_antonyms(word):\n",
    "    # Get synsets for the word\n",
    "    synsets = wn.synsets(word)\n",
    "    \n",
    "    # List to store antonyms\n",
    "    antonyms = set()\n",
    "\n",
    "    # Iterate over synsets\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            # Check if the lemma has an antonym\n",
    "            if lemma.antonyms():\n",
    "                antonyms.add(lemma.antonyms()[0].name())  # Add the antonym to the set\n",
    "    \n",
    "    return list(antonyms)\n",
    "\n",
    "# Example: Get antonyms for the word \"happy\"\n",
    "word = \"happy\"\n",
    "antonyms = get_antonyms(word)\n",
    "\n",
    "# Print the antonyms\n",
    "print(f\"Antonyms for '{word}':\")\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fc935627-2e63-4cec-a917-77e8f4e2bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=wn.synsets('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5a3ce074-b8dd-4492-b3a0-d5d3144daa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unhappy\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    z=i.lemmas()\n",
    "    for i in z:\n",
    "        t=i.antonyms()\n",
    "        if t:\n",
    "           print(t[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c6005830-8f00-42f1-9e36-d101bd91ccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words and their POS tags:\n",
      "The: DT\n",
      "quick: JJ\n",
      "brown: NN\n",
      "fox: NN\n",
      "jumped: VBD\n",
      "over: IN\n",
      "the: DT\n",
      "lazy: JJ\n",
      "dog: NN\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Apply POS Tagging\n",
    "pos_tags = pos_tag(words)\n",
    "\n",
    "# Print POS tags\n",
    "print(\"Words and their POS tags:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word}: {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ef513730-0a69-40fb-b521-1decbf442420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumped', 'VBD'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0a986b0e-42fc-4c91-b672-e4d2b2b99ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4a83b8e2-2aea-4349-82c5-82e5e84eb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5a60ab5b-bc43-4c0d-83a8-adfe34386630",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"books quick brown fox jumped over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c2166edd-c262-4e64-b669-7cb56e576e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0e522b5a-e2d3-47c5-aedd-424a7a8bcb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books : book : NNS\n",
      "quick : quick : JJ\n",
      "brown : brown : JJ\n",
      "fox : fox : NN\n",
      "jumped : jumped : VBD\n",
      "over : over : IN\n",
      "the : the : DT\n",
      "lazy : lazy : JJ\n",
      "dog : dog : NN\n",
      ". : . : .\n"
     ]
    }
   ],
   "source": [
    "for k,i in enumerate(words):\n",
    "    print(i,':',z.lemmatize(i),':',l[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2bd43785-b127-4a29-a2f3-f082d0b62199",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0703b41d-81b1-4581-a673-d0c413cb0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in z:\n",
    "    l.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "386726b4-1740-4b45-bef7-7e56bfb13820",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e5d695f0-ff67-45a2-85a6-c77fbec52220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NNS', 'JJ', 'JJ', 'NN', 'VBD', 'IN', 'DT', 'JJ', 'NN', '.']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968a2396-7b3c-4f37-9671-0177c3982c84",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load spaCy's English model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Process the sentence\u001b[39;00m\n\u001b[0;32m      7\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cats were playing with the dog\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms ball near the park.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(\"The cats were playing with the dog's ball near the park.\")\n",
    "\n",
    "# Print detailed POS tags\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_}, {token.tag_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f2125a-45e4-4fc7-b061-672bd9674c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f2eed5c5-b399-4208-b3fe-9b9ecc498380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/11.8 MB 3.2 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.1/11.8 MB 3.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.9/11.8 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.7/11.8 MB 3.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.5/11.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/11.8 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.0/11.8 MB 3.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.8/11.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.6/11.8 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.1/11.8 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.9/11.8 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/11.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.8 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.8/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.1/6.3 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.0/5.4 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.1/5.4 MB 5.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.9/5.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.2/5.4 MB 4.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 shellingham-1.5.4 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.1 wasabi-1.1.3 weasel-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script typer.exe is installed in 'C:\\Users\\barad\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script weasel.exe is installed in 'C:\\Users\\barad\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\barad\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12baf2d3-4a58-40aa-9a6f-cea5d246d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\barad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram model - Similar words for 'word':\n",
      "[('popular', 0.22802448272705078), ('use', 0.20308668911457062), ('we', 0.19010120630264282), ('classification', 0.1672562062740326), ('cbow', 0.13043780624866486)]\n",
      "\n",
      "CBOW model - Similar words for 'word':\n",
      "[('popular', 0.22814925014972687), ('use', 0.20323406159877777), ('we', 0.19010120630264282), ('classification', 0.16729910671710968), ('cbow', 0.13043780624866486)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text (corpus) to train the Word2Vec models\n",
    "corpus = [\n",
    "    \"Natural language processing is a field of artificial intelligence.\",\n",
    "    \"Word2Vec is a powerful tool for creating word embeddings.\",\n",
    "    \"We can use word embeddings for various NLP tasks like classification and sentiment analysis.\",\n",
    "    \"Skip-gram and CBOW are two popular approaches to train Word2Vec models.\"\n",
    "]\n",
    "\n",
    "# Tokenizing the corpus\n",
    "tokens = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# 1. *Skip-gram Model*: \n",
    "# In Skip-gram, we try to predict the surrounding context words for a given center word.\n",
    "\n",
    "skipgram_model = Word2Vec(sentences=tokens, vector_size=50, window=5, min_count=1, sg=1, workers=4)\n",
    "\n",
    "# 2. *CBOW Model*: \n",
    "# In CBOW, we predict the center word from the context words.\n",
    "\n",
    "cbow_model = Word2Vec(sentences=tokens, vector_size=50, window=5, min_count=1, sg=0, workers=4)\n",
    "\n",
    "\n",
    "\n",
    "# Skip-gram: Find similar words for the word \"word\"\n",
    "print(\"Skip-gram model - Similar words for 'word':\")\n",
    "print(skipgram_model.wv.most_similar('word', topn=5))\n",
    "\n",
    "# CBOW: Find similar words for the word \"word\"\n",
    "print(\"\\nCBOW model - Similar words for 'word':\")\n",
    "print(cbow_model.wv.most_similar('word', topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d9604-4b43-4a30-9221-086f75a2e6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
